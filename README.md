# Project Topic: Residual Attention Network for Image Classification
This project is a part of ECBM E4040: Neural Networks and Deep Learning coursework taught by Prof. Zoran Kostic.

# Project Description
In this project, we have compared the performance of various Attention models (Naive Attention, Attention-56, Attention-92) with standard neural network architectures for classifying images on two datasets namely CIFAR-10 and CIFAR-100. Each of the .ipynb notebooks represent our implementation for that particular architecture. We have saved our trained models in the the saved_models folder. 

# Acknowledgements
We would like to thank Prof. Zoran Kostic and the teaching assistants for their guidance and resources provided to us to complete this project. 

# Organization of this directory
```
├── Final Project Report.pdf
├── README.md
├── cifar100_Attention56.ipynb
├── cifar100_Attention92.ipynb
├── cifar100_resnet152.ipynb
├── cifar10_Attention56_NAL.ipynb
├── cifar10_Attention92.ipynb
├── cifar10_attention56.ipynb
├── cifar10_resnet152.ipynb
├── cifar_naivecnn.ipynb
├── figures
│   ├── VM-instance-on-gcp.jpg
│   ├── attention56-running-on-gcp.jpg
│   ├── attention92-running-on-gcp.jpg
│   ├── cifar10-attention56-running-on-gcp.jpg
│   └── resnet152-running-on-gcp.jpg
└── utils
    ├── architectures.py
    ├── plot.py
    └── residual_attention_network.py
2 directories, 18 files

```
